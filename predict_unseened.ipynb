{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "de8fc2d7-1531-4afc-b57d-829dc545f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from ultralytics.data.augment import Albumentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "3bb83b18-8a86-4d5f-858a-fc5b31f14fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, p=1.0):\n",
    "    \"\"\"Initialize the transform object for YOLO bbox formatted params.\"\"\"\n",
    "    self.p = p\n",
    "    self.transform = None\n",
    "    prefix = colorstr(\"albumentations: \")\n",
    "    try:\n",
    "        import albumentations as A\n",
    "\n",
    "        # check_version(A.__version__, \"1.0.3\", hard=True)  # version requirement\n",
    "\n",
    "        # Transforms\n",
    "        T = [\n",
    "            A.Rotate(limit = 10, p=0.0),\n",
    "            A.Blur(p=0.0),\n",
    "            A.MedianBlur(p=0.0),\n",
    "            A.ToGray(p=0.0),\n",
    "            A.CLAHE(p=0.0),\n",
    "            # A.RandomBrightnessContrast(brightness_limit=(-0.25, -0.25), contrast_limit=(-0.25, -0.25), p=0),\n",
    "            A.RandomGamma(p=0.0),\n",
    "            A.ImageCompression(quality_lower=75, p=0.0),\n",
    "\n",
    "        ]\n",
    "        self.contains_spatial = any(transform.__class__.__name__ in spatial_transforms for transform in T)\n",
    "        self.transform = (\n",
    "            A.Compose(T, bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"class_labels\"]))\n",
    "            if self.contains_spatial\n",
    "            else A.Compose(T)\n",
    "        )\n",
    "\n",
    "        LOGGER.info(prefix + \", \".join(f\"{x}\".replace(\"always_apply=False, \", \"\") for x in T if x.p))\n",
    "    except ImportError:  # package not installed, skip\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        LOGGER.info(f\"{prefix}{e}\")\n",
    "\n",
    "\n",
    "Albumentations.__init__ = __init__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1693a087-366b-43f2-8236-020777f034a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2363/2363 [01:57<00:00, 20.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0017444981213097155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# inference(test) code\n",
    "# train26 : goat\n",
    "\n",
    "# test -> re-bbox -> completed (urin_by_class)\n",
    "\n",
    "'''\n",
    "model = YOLO(\"runs/detect/train26/weights/best.pt\", verbose=False)\n",
    "\n",
    "# data_path = \"/home/jupyter-heypaprika/datasets/urin_check_dataset/Z-FLIP_unlabelled/\"\n",
    "data_path = \"/home/jupyter-heypaprika/input/urin_by_class/*/*/*\"\n",
    "files = glob.glob(data_path)\n",
    "total = 0\n",
    "total_wrong = 0\n",
    "for file in tqdm(files):\n",
    "    if not \"jpg\" in file:\n",
    "        continue\n",
    "    results = model(file, imgsz=224, verbose=False)\n",
    "    boxes = results[0].boxes\n",
    "    cur_cls = map(int, boxes.cls.tolist())\n",
    "    cur_xywhn = boxes.xywhn.cpu().numpy()\n",
    "    cur_xywhn = np.array([[\"{:.6f}\".format(x) for x in row] for row in cur_xywhn])\n",
    "    cur_list = []\n",
    "    for cls, xywhn in zip(cur_cls, cur_xywhn):\n",
    "        xywhn = np.insert(xywhn, 0, str(cls))\n",
    "        cur_list.append(xywhn)\n",
    "    \n",
    "    text_data = \"\\n\".join([\" \".join(row) for row in cur_list])\n",
    "    \n",
    "    # Save to a .txt file\n",
    "    file_path = file.replace(file[-4:], \".txt\")\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(text_data)\n",
    "    predict_value = 60\n",
    "    if len(cur_list) <= predict_value:\n",
    "        len_boxes = len(cur_list)\n",
    "        wrong = predict_value - len_boxes\n",
    "        # print(\"total:\", len_boxes, \", wrong:\", wrong)\n",
    "        total += predict_value\n",
    "        total_wrong += wrong\n",
    "\n",
    "avg_wrong = total_wrong / total\n",
    "print(avg_wrong)\n",
    "\n",
    "\n",
    "# results[0].show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4aa75d6f-68f2-4cbc-adc4-09a05ef61bae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-heypaprika/datasets/urin_check_dataset/Z-FLIP_unlabelled/20241015_165447.jpg\n",
      "[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "conf: tensor([0.9074, 0.9056, 0.9033, 0.8948, 0.8940, 0.8902, 0.8865, 0.8837, 0.8824, 0.8823, 0.8816, 0.8772, 0.8729, 0.8723, 0.8720, 0.8718, 0.8705, 0.8702, 0.8690, 0.8667, 0.8652, 0.8645, 0.8644, 0.8640, 0.8626, 0.8620, 0.8617, 0.8611, 0.8595, 0.8585, 0.8538, 0.8526, 0.8476, 0.8472, 0.8455, 0.8440], device='cuda:0')\n",
      "data: tensor([[3.9252e+02, 1.0020e+03, 3.8274e+03, 1.1792e+03, 9.0741e-01, 0.0000e+00],\n",
      "        [4.2550e+02, 1.9297e+03, 3.8401e+03, 2.1145e+03, 9.0562e-01, 0.0000e+00],\n",
      "        [3.8332e+02, 1.4708e+03, 3.8090e+03, 1.6798e+03, 9.0331e-01, 0.0000e+00],\n",
      "        [9.0413e+02, 1.9695e+03, 1.0452e+03, 2.0991e+03, 8.9485e-01, 1.0000e+00],\n",
      "        [1.1090e+03, 1.9696e+03, 1.2497e+03, 2.0982e+03, 8.9404e-01, 1.0000e+00],\n",
      "        [4.7410e+02, 1.0283e+03, 6.1555e+02, 1.1548e+03, 8.9024e-01, 1.0000e+00],\n",
      "        [1.3138e+03, 1.9685e+03, 1.4537e+03, 2.0974e+03, 8.8646e-01, 1.0000e+00],\n",
      "        [1.5171e+03, 1.9672e+03, 1.6593e+03, 2.0983e+03, 8.8366e-01, 1.0000e+00],\n",
      "        [1.8918e+03, 1.5184e+03, 2.0253e+03, 1.6409e+03, 8.8235e-01, 1.0000e+00],\n",
      "        [6.5265e+02, 1.5405e+03, 7.9235e+02, 1.6682e+03, 8.8234e-01, 1.0000e+00],\n",
      "        [2.5489e+03, 1.9576e+03, 2.6896e+03, 2.0877e+03, 8.8160e-01, 1.0000e+00],\n",
      "        [1.6822e+03, 1.5231e+03, 1.8212e+03, 1.6475e+03, 8.7722e-01, 1.0000e+00],\n",
      "        [1.4794e+03, 1.5279e+03, 1.6144e+03, 1.6519e+03, 8.7286e-01, 1.0000e+00],\n",
      "        [6.7475e+02, 1.0269e+03, 8.1538e+02, 1.1539e+03, 8.7230e-01, 1.0000e+00],\n",
      "        [2.1224e+03, 1.0205e+03, 2.2654e+03, 1.1519e+03, 8.7199e-01, 1.0000e+00],\n",
      "        [2.3315e+03, 1.0232e+03, 2.4731e+03, 1.1528e+03, 8.7177e-01, 1.0000e+00],\n",
      "        [2.3430e+03, 1.9591e+03, 2.4822e+03, 2.0885e+03, 8.7047e-01, 1.0000e+00],\n",
      "        [2.0940e+03, 1.5133e+03, 2.2318e+03, 1.6387e+03, 8.7024e-01, 1.0000e+00],\n",
      "        [1.9314e+03, 1.9647e+03, 2.0714e+03, 2.0931e+03, 8.6900e-01, 1.0000e+00],\n",
      "        [5.0020e+02, 1.9700e+03, 6.4155e+02, 2.0994e+03, 8.6671e-01, 1.0000e+00],\n",
      "        [1.2709e+03, 1.5304e+03, 1.4083e+03, 1.6548e+03, 8.6517e-01, 1.0000e+00],\n",
      "        [4.4836e+02, 1.5399e+03, 5.9127e+02, 1.6717e+03, 8.6448e-01, 1.0000e+00],\n",
      "        [1.7257e+03, 1.9662e+03, 1.8674e+03, 2.0949e+03, 8.6443e-01, 1.0000e+00],\n",
      "        [7.0036e+02, 1.9697e+03, 8.4113e+02, 2.0988e+03, 8.6400e-01, 1.0000e+00],\n",
      "        [1.9168e+03, 1.0213e+03, 2.0578e+03, 1.1514e+03, 8.6261e-01, 1.0000e+00],\n",
      "        [8.5731e+02, 1.5379e+03, 9.9644e+02, 1.6640e+03, 8.6196e-01, 1.0000e+00],\n",
      "        [8.8042e+02, 1.0242e+03, 1.0219e+03, 1.1534e+03, 8.6173e-01, 1.0000e+00],\n",
      "        [1.0880e+03, 1.0225e+03, 1.2312e+03, 1.1529e+03, 8.6108e-01, 1.0000e+00],\n",
      "        [2.5346e+03, 1.0234e+03, 2.6767e+03, 1.1533e+03, 8.5947e-01, 1.0000e+00],\n",
      "        [2.3009e+03, 1.5100e+03, 2.4408e+03, 1.6369e+03, 8.5852e-01, 1.0000e+00],\n",
      "        [2.1358e+03, 1.9628e+03, 2.2763e+03, 2.0901e+03, 8.5380e-01, 1.0000e+00],\n",
      "        [1.4994e+03, 1.0257e+03, 1.6422e+03, 1.1567e+03, 8.5255e-01, 1.0000e+00],\n",
      "        [1.7106e+03, 1.0229e+03, 1.8527e+03, 1.1547e+03, 8.4756e-01, 1.0000e+00],\n",
      "        [1.0642e+03, 1.5337e+03, 1.2037e+03, 1.6603e+03, 8.4721e-01, 1.0000e+00],\n",
      "        [1.2955e+03, 1.0233e+03, 1.4366e+03, 1.1523e+03, 8.4550e-01, 1.0000e+00],\n",
      "        [2.5061e+03, 1.5076e+03, 2.6471e+03, 1.6347e+03, 8.4401e-01, 1.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (3000, 4000)\n",
      "shape: torch.Size([36, 6])\n",
      "xywh: tensor([[2109.9624, 1090.5791, 3434.8760,  177.1448],\n",
      "        [2132.7837, 2022.1075, 3414.5591,  184.8054],\n",
      "        [2096.1387, 1575.3058, 3425.6323,  208.9207],\n",
      "        [ 974.6808, 2034.3049,  141.1104,  129.6382],\n",
      "        [1179.3547, 2033.8782,  140.7654,  128.6392],\n",
      "        [ 544.8246, 1091.5488,  141.4509,  126.5972],\n",
      "        [1383.7625, 2032.9402,  139.9545,  128.8662],\n",
      "        [1588.2163, 2032.7625,  142.2159,  131.1654],\n",
      "        [1958.5553, 1579.6357,  133.4768,  122.4319],\n",
      "        [ 722.4991, 1604.3772,  139.6999,  127.7180],\n",
      "        [2619.2261, 2022.6478,  140.6794,  130.1365],\n",
      "        [1751.6936, 1585.3088,  138.9972,  124.4049],\n",
      "        [1546.8656, 1589.8741,  134.9932,  123.9700],\n",
      "        [ 745.0647, 1090.3804,  140.6342,  126.9965],\n",
      "        [2193.8687, 1086.2233,  143.0027,  131.3792],\n",
      "        [2402.3240, 1087.9929,  141.5635,  129.5367],\n",
      "        [2412.6138, 2023.8335,  139.1421,  129.4180],\n",
      "        [2162.8779, 1575.9854,  137.7639,  125.4463],\n",
      "        [2001.4026, 2028.9053,  139.9966,  128.4330],\n",
      "        [ 570.8744, 2034.7017,  141.3444,  129.4834],\n",
      "        [1339.6063, 1592.6284,  137.3906,  124.4271],\n",
      "        [ 519.8150, 1605.7970,  142.9059,  131.8574],\n",
      "        [1796.5454, 2030.5806,  141.7885,  128.6782],\n",
      "        [ 770.7447, 2034.2229,  140.7751,  129.1056],\n",
      "        [1987.3193, 1086.3328,  141.0446,  130.1613],\n",
      "        [ 926.8746, 1600.9592,  139.1219,  126.1599],\n",
      "        [ 951.1518, 1088.8026,  141.4702,  129.1855],\n",
      "        [1159.6016, 1087.6837,  143.2931,  130.3853],\n",
      "        [2605.6255, 1088.3951,  142.1006,  129.9057],\n",
      "        [2370.8354, 1573.4333,  139.8953,  126.9663],\n",
      "        [2206.0137, 2026.4508,  140.5249,  127.3157],\n",
      "        [1570.7878, 1091.2235,  142.7988,  131.0125],\n",
      "        [1781.6541, 1088.7607,  142.1918,  131.7931],\n",
      "        [1133.9852, 1597.0027,  139.4961,  126.5651],\n",
      "        [1366.0842, 1087.7859,  141.0963,  128.9929],\n",
      "        [2576.6001, 1571.1400,  140.9399,  127.0938]], device='cuda:0')\n",
      "xywhn: tensor([[0.5275, 0.3635, 0.8587, 0.0590],\n",
      "        [0.5332, 0.6740, 0.8536, 0.0616],\n",
      "        [0.5240, 0.5251, 0.8564, 0.0696],\n",
      "        [0.2437, 0.6781, 0.0353, 0.0432],\n",
      "        [0.2948, 0.6780, 0.0352, 0.0429],\n",
      "        [0.1362, 0.3638, 0.0354, 0.0422],\n",
      "        [0.3459, 0.6776, 0.0350, 0.0430],\n",
      "        [0.3971, 0.6776, 0.0356, 0.0437],\n",
      "        [0.4896, 0.5265, 0.0334, 0.0408],\n",
      "        [0.1806, 0.5348, 0.0349, 0.0426],\n",
      "        [0.6548, 0.6742, 0.0352, 0.0434],\n",
      "        [0.4379, 0.5284, 0.0347, 0.0415],\n",
      "        [0.3867, 0.5300, 0.0337, 0.0413],\n",
      "        [0.1863, 0.3635, 0.0352, 0.0423],\n",
      "        [0.5485, 0.3621, 0.0358, 0.0438],\n",
      "        [0.6006, 0.3627, 0.0354, 0.0432],\n",
      "        [0.6032, 0.6746, 0.0348, 0.0431],\n",
      "        [0.5407, 0.5253, 0.0344, 0.0418],\n",
      "        [0.5004, 0.6763, 0.0350, 0.0428],\n",
      "        [0.1427, 0.6782, 0.0353, 0.0432],\n",
      "        [0.3349, 0.5309, 0.0343, 0.0415],\n",
      "        [0.1300, 0.5353, 0.0357, 0.0440],\n",
      "        [0.4491, 0.6769, 0.0354, 0.0429],\n",
      "        [0.1927, 0.6781, 0.0352, 0.0430],\n",
      "        [0.4968, 0.3621, 0.0353, 0.0434],\n",
      "        [0.2317, 0.5337, 0.0348, 0.0421],\n",
      "        [0.2378, 0.3629, 0.0354, 0.0431],\n",
      "        [0.2899, 0.3626, 0.0358, 0.0435],\n",
      "        [0.6514, 0.3628, 0.0355, 0.0433],\n",
      "        [0.5927, 0.5245, 0.0350, 0.0423],\n",
      "        [0.5515, 0.6755, 0.0351, 0.0424],\n",
      "        [0.3927, 0.3637, 0.0357, 0.0437],\n",
      "        [0.4454, 0.3629, 0.0355, 0.0439],\n",
      "        [0.2835, 0.5323, 0.0349, 0.0422],\n",
      "        [0.3415, 0.3626, 0.0353, 0.0430],\n",
      "        [0.6442, 0.5237, 0.0352, 0.0424]], device='cuda:0')\n",
      "xyxy: tensor([[ 392.5244, 1002.0067, 3827.4004, 1179.1515],\n",
      "        [ 425.5041, 1929.7048, 3840.0632, 2114.5103],\n",
      "        [ 383.3225, 1470.8455, 3808.9548, 1679.7661],\n",
      "        [ 904.1255, 1969.4858, 1045.2360, 2099.1240],\n",
      "        [1108.9720, 1969.5586, 1249.7374, 2098.1978],\n",
      "        [ 474.0992, 1028.2502,  615.5500, 1154.8474],\n",
      "        [1313.7852, 1968.5071, 1453.7396, 2097.3733],\n",
      "        [1517.1084, 1967.1798, 1659.3243, 2098.3452],\n",
      "        [1891.8169, 1518.4198, 2025.2937, 1640.8517],\n",
      "        [ 652.6492, 1540.5182,  792.3491, 1668.2362],\n",
      "        [2548.8865, 1957.5796, 2689.5659, 2087.7161],\n",
      "        [1682.1949, 1523.1063, 1821.1921, 1647.5112],\n",
      "        [1479.3690, 1527.8892, 1614.3622, 1651.8591],\n",
      "        [ 674.7476, 1026.8822,  815.3818, 1153.8787],\n",
      "        [2122.3674, 1020.5336, 2265.3701, 1151.9128],\n",
      "        [2331.5422, 1023.2246, 2473.1057, 1152.7614],\n",
      "        [2343.0427, 1959.1245, 2482.1848, 2088.5425],\n",
      "        [2093.9961, 1513.2622, 2231.7600, 1638.7085],\n",
      "        [1931.4043, 1964.6888, 2071.4009, 2093.1218],\n",
      "        [ 500.2022, 1969.9600,  641.5466, 2099.4434],\n",
      "        [1270.9110, 1530.4148, 1408.3016, 1654.8419],\n",
      "        [ 448.3620, 1539.8683,  591.2679, 1671.7257],\n",
      "        [1725.6512, 1966.2415, 1867.4397, 2094.9197],\n",
      "        [ 700.3571, 1969.6700,  841.1323, 2098.7756],\n",
      "        [1916.7970, 1021.2521, 2057.8416, 1151.4135],\n",
      "        [ 857.3137, 1537.8793,  996.4356, 1664.0392],\n",
      "        [ 880.4167, 1024.2098, 1021.8869, 1153.3954],\n",
      "        [1087.9551, 1022.4911, 1231.2482, 1152.8763],\n",
      "        [2534.5752, 1023.4423, 2676.6758, 1153.3480],\n",
      "        [2300.8877, 1509.9502, 2440.7830, 1636.9165],\n",
      "        [2135.7512, 1962.7930, 2276.2761, 2090.1086],\n",
      "        [1499.3884, 1025.7173, 1642.1873, 1156.7297],\n",
      "        [1710.5581, 1022.8641, 1852.7499, 1154.6572],\n",
      "        [1064.2372, 1533.7202, 1203.7333, 1660.2853],\n",
      "        [1295.5361, 1023.2895, 1436.6324, 1152.2823],\n",
      "        [2506.1301, 1507.5931, 2647.0701, 1634.6869]], device='cuda:0')\n",
      "xyxyn: tensor([[0.0981, 0.3340, 0.9569, 0.3931],\n",
      "        [0.1064, 0.6432, 0.9600, 0.7048],\n",
      "        [0.0958, 0.4903, 0.9522, 0.5599],\n",
      "        [0.2260, 0.6565, 0.2613, 0.6997],\n",
      "        [0.2772, 0.6565, 0.3124, 0.6994],\n",
      "        [0.1185, 0.3428, 0.1539, 0.3849],\n",
      "        [0.3284, 0.6562, 0.3634, 0.6991],\n",
      "        [0.3793, 0.6557, 0.4148, 0.6994],\n",
      "        [0.4730, 0.5061, 0.5063, 0.5470],\n",
      "        [0.1632, 0.5135, 0.1981, 0.5561],\n",
      "        [0.6372, 0.6525, 0.6724, 0.6959],\n",
      "        [0.4205, 0.5077, 0.4553, 0.5492],\n",
      "        [0.3698, 0.5093, 0.4036, 0.5506],\n",
      "        [0.1687, 0.3423, 0.2038, 0.3846],\n",
      "        [0.5306, 0.3402, 0.5663, 0.3840],\n",
      "        [0.5829, 0.3411, 0.6183, 0.3843],\n",
      "        [0.5858, 0.6530, 0.6205, 0.6962],\n",
      "        [0.5235, 0.5044, 0.5579, 0.5462],\n",
      "        [0.4829, 0.6549, 0.5179, 0.6977],\n",
      "        [0.1251, 0.6567, 0.1604, 0.6998],\n",
      "        [0.3177, 0.5101, 0.3521, 0.5516],\n",
      "        [0.1121, 0.5133, 0.1478, 0.5572],\n",
      "        [0.4314, 0.6554, 0.4669, 0.6983],\n",
      "        [0.1751, 0.6566, 0.2103, 0.6996],\n",
      "        [0.4792, 0.3404, 0.5145, 0.3838],\n",
      "        [0.2143, 0.5126, 0.2491, 0.5547],\n",
      "        [0.2201, 0.3414, 0.2555, 0.3845],\n",
      "        [0.2720, 0.3408, 0.3078, 0.3843],\n",
      "        [0.6336, 0.3411, 0.6692, 0.3844],\n",
      "        [0.5752, 0.5033, 0.6102, 0.5456],\n",
      "        [0.5339, 0.6543, 0.5691, 0.6967],\n",
      "        [0.3748, 0.3419, 0.4105, 0.3856],\n",
      "        [0.4276, 0.3410, 0.4632, 0.3849],\n",
      "        [0.2661, 0.5112, 0.3009, 0.5534],\n",
      "        [0.3239, 0.3411, 0.3592, 0.3841],\n",
      "        [0.6265, 0.5025, 0.6618, 0.5449]], device='cuda:0')\n",
      "(3000, 4000, 3)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/home/jupyter-heypaprika/datasets/urin_check_dataset/Z-FLIP_unlabelled/\"\n",
    "files = glob.glob(data_path+\"*.jpg\")\n",
    "file = files[0]\n",
    "print(file)\n",
    "results = model(file, imgsz=224, verbose=False)\n",
    "boxes = results[0].boxes\n",
    "cur_cls = list(map(int, boxes.cls.tolist()))\n",
    "cur_xywhn = boxes.xywhn.cpu().numpy()\n",
    "cur_xywhn = np.array([[\"{:.6f}\".format(x) for x in row] for row in cur_xywhn])\n",
    "cur_list = []\n",
    "print(cur_cls)\n",
    "print(boxes)\n",
    "# for cls, xywhn in zip(cur_cls, cur_xywhn):\n",
    "print(results[0].orig_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "bbdcb8c8-8a40-44f8-9737-7804bb8ba009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kit 1\n",
      "[     392.52        1002      3827.4      1179.2]\n",
      "Kit 1에 해당하는 sensor 데이터:\n",
      "[      474.1      1028.3      615.55      1154.8]\n",
      "[     674.75      1026.9      815.38      1153.9]\n",
      "[     880.42      1024.2      1021.9      1153.4]\n",
      "[       1088      1022.5      1231.2      1152.9]\n",
      "[     1295.5      1023.3      1436.6      1152.3]\n",
      "[     1499.4      1025.7      1642.2      1156.7]\n",
      "[     1710.6      1022.9      1852.7      1154.7]\n",
      "[     1916.8      1021.3      2057.8      1151.4]\n",
      "[     2122.4      1020.5      2265.4      1151.9]\n",
      "[     2331.5      1023.2      2473.1      1152.8]\n",
      "[     2534.6      1023.4      2676.7      1153.3]\n",
      "저장: dashboard_000.jpg\n",
      "Kit 2\n",
      "[     383.32      1470.8        3809      1679.8]\n",
      "Kit 2에 해당하는 sensor 데이터:\n",
      "[     448.36      1539.9      591.27      1671.7]\n",
      "[     652.65      1540.5      792.35      1668.2]\n",
      "[     857.31      1537.9      996.44        1664]\n",
      "[     1064.2      1533.7      1203.7      1660.3]\n",
      "[     1270.9      1530.4      1408.3      1654.8]\n",
      "[     1479.4      1527.9      1614.4      1651.9]\n",
      "[     1682.2      1523.1      1821.2      1647.5]\n",
      "[     1891.8      1518.4      2025.3      1640.9]\n",
      "[       2094      1513.3      2231.8      1638.7]\n",
      "[     2300.9        1510      2440.8      1636.9]\n",
      "[     2506.1      1507.6      2647.1      1634.7]\n",
      "저장: dashboard_001.jpg\n",
      "Kit 3\n",
      "[      425.5      1929.7      3840.1      2114.5]\n",
      "Kit 3에 해당하는 sensor 데이터:\n",
      "[      500.2        1970      641.55      2099.4]\n",
      "[     700.36      1969.7      841.13      2098.8]\n",
      "[     904.13      1969.5      1045.2      2099.1]\n",
      "[       1109      1969.6      1249.7      2098.2]\n",
      "[     1313.8      1968.5      1453.7      2097.4]\n",
      "[     1517.1      1967.2      1659.3      2098.3]\n",
      "[     1725.7      1966.2      1867.4      2094.9]\n",
      "[     1931.4      1964.7      2071.4      2093.1]\n",
      "[     2135.8      1962.8      2276.3      2090.1]\n",
      "[       2343      1959.1      2482.2      2088.5]\n",
      "[     2548.9      1957.6      2689.6      2087.7]\n",
      "저장: dashboard_002.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# 잘라내기 (이거 완성 후에는 모델 내에서 잘라내는 것으로)\n",
    "V, H, c = results[0].orig_img.shape\n",
    "xyxy = boxes.xyxy.cpu().numpy()\n",
    "if xyxy.shape[0] != 36:\n",
    "    pass\n",
    "    #continue\n",
    "\n",
    "# xyxy = np.concatenate([xyxy, np.array(cur_cls).reshape(-1,1)], axis=1)\n",
    "sorted_data = xyxy[np.lexsort((xyxy[:,1], xyxy[:,0]))]\n",
    "\n",
    "kit = sorted_data[:3]\n",
    "sensor = sorted_data[3:]\n",
    "kit = kit[np.lexsort((kit[:,0], kit[:,1]))]\n",
    "groups = []\n",
    "\n",
    "for k in kit:\n",
    "    # k[0]: lower bound for x, k[2]: upper bound for x\n",
    "    # k[1]: lower bound for y, k[3]: upper bound for y\n",
    "    mask = (sensor[:, 0] > k[0]) & (sensor[:, 0] < k[2]) & \\\n",
    "           (sensor[:, 1] > k[1]) & (sensor[:, 1] < k[3])\n",
    "    result = sensor[mask]\n",
    "    result = result[np.lexsort((result[:,1], result[:,0]))]\n",
    "\n",
    "    groups.append(result)\n",
    "\n",
    "RESIZE_SIZE = 64          # 개별 이미지 크기 (64x64)\n",
    "GRID_ROWS = 4             # 그리드 행 수\n",
    "GRID_COLS = 3             # 그리드 열 수\n",
    "COMPOSITE_SIZE_ROW = RESIZE_SIZE * GRID_ROWS  # 최종 대시보드 이미지 크기 \n",
    "COMPOSITE_SIZE_COL = RESIZE_SIZE * GRID_COLS  # 최종 대시보드 이미지 크기 \n",
    "\n",
    "\n",
    "# 각 그룹 출력\n",
    "for idx, (kit, group) in enumerate(zip(kit, groups)):\n",
    "    print(f\"Kit {idx+1}\")\n",
    "    print(kit)\n",
    "    print(f\"Kit {idx+1}에 해당하는 sensor 데이터:\")\n",
    "    # print(group, \"\\n\")\n",
    "    # 6x6 그리드 composite 이미지 생성 (배경: 검은색)\n",
    "    composite_img = Image.new(\"RGB\", (COMPOSITE_SIZE_COL, COMPOSITE_SIZE_ROW), color=(0, 0, 0))\n",
    "    # cur_kit_images = []\n",
    "    for j, patch in enumerate(group):\n",
    "        print(patch)\n",
    "        x1, y1, x2, y2 = patch\n",
    "        crop_patch = results[0].orig_img[round(y1):round(y2), round(x1):round(x2)]\n",
    "        # crop_patch = cv2.cvtColor(crop_patch.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # cur_kit_images.append(crop_patch)\n",
    "    \n",
    "        # for j, file_path in enumerate(composite_files):\n",
    "        row = j // GRID_COLS\n",
    "        col = j % GRID_COLS\n",
    "        x_offset = col * RESIZE_SIZE\n",
    "        y_offset = row * RESIZE_SIZE\n",
    "        crop_patch = crop_patch[..., ::-1]\n",
    "        # v1: 전체 이미지를 그대로 리사이징\n",
    "        crop_patch_pil = Image.fromarray(crop_patch)\n",
    "        crop_patch_pil = crop_patch_pil.resize((RESIZE_SIZE, RESIZE_SIZE))\n",
    "    \n",
    "        composite_img.paste(crop_patch_pil, (x_offset, y_offset))\n",
    "        \n",
    "    out_filename = f\"dashboard_{idx:03d}.jpg\"\n",
    "    out_path = os.path.join(out_filename)\n",
    "    composite_img.save(out_path)\n",
    "    print(f\"저장: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1c2fe459-8073-4e58-b65c-f1fe4bb8c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur Kit 에 해당하는 sensor 데이터:\n",
      "[      474.1      1028.3      615.55      1154.8]\n",
      "[     674.75      1026.9      815.38      1153.9]\n",
      "[     880.42      1024.2      1021.9      1153.4]\n",
      "[       1088      1022.5      1231.2      1152.9]\n",
      "[     1295.5      1023.3      1436.6      1152.3]\n",
      "[     1499.4      1025.7      1642.2      1156.7]\n",
      "[     1710.6      1022.9      1852.7      1154.7]\n",
      "[     1916.8      1021.3      2057.8      1151.4]\n",
      "[     2122.4      1020.5      2265.4      1151.9]\n",
      "[     2331.5      1023.2      2473.1      1152.8]\n",
      "[     2534.6      1023.4      2676.7      1153.3]\n",
      "cur Kit 에 해당하는 sensor 데이터:\n",
      "[     448.36      1539.9      591.27      1671.7]\n",
      "[     652.65      1540.5      792.35      1668.2]\n",
      "[     857.31      1537.9      996.44        1664]\n",
      "[     1064.2      1533.7      1203.7      1660.3]\n",
      "[     1270.9      1530.4      1408.3      1654.8]\n",
      "[     1479.4      1527.9      1614.4      1651.9]\n",
      "[     1682.2      1523.1      1821.2      1647.5]\n",
      "[     1891.8      1518.4      2025.3      1640.9]\n",
      "[       2094      1513.3      2231.8      1638.7]\n",
      "[     2300.9        1510      2440.8      1636.9]\n",
      "[     2506.1      1507.6      2647.1      1634.7]\n",
      "cur Kit 에 해당하는 sensor 데이터:\n",
      "[      500.2        1970      641.55      2099.4]\n",
      "[     700.36      1969.7      841.13      2098.8]\n",
      "[     904.13      1969.5      1045.2      2099.1]\n",
      "[       1109      1969.6      1249.7      2098.2]\n",
      "[     1313.8      1968.5      1453.7      2097.4]\n",
      "[     1517.1      1967.2      1659.3      2098.3]\n",
      "[     1725.7      1966.2      1867.4      2094.9]\n",
      "[     1931.4      1964.7      2071.4      2093.1]\n",
      "[     2135.8      1962.8      2276.3      2090.1]\n",
      "[       2343      1959.1      2482.2      2088.5]\n",
      "[     2548.9      1957.6      2689.6      2087.7]\n",
      "torch.Size([3, 11, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "image_size = 32\n",
    "\n",
    "transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),  # Converts to (C, H, W)\n",
    "        ])\n",
    "\n",
    "\n",
    "# 각 그룹 출력\n",
    "batch = []\n",
    "for group in groups:\n",
    "    print(f\"cur Kit 에 해당하는 sensor 데이터:\")\n",
    "    images = []\n",
    "    for j, patch in enumerate(group):\n",
    "        print(patch)\n",
    "        x1, y1, x2, y2 = patch\n",
    "        crop_patch = results[0].orig_img[round(y1):round(y2), round(x1):round(x2)]\n",
    "        crop_patch = crop_patch[..., ::-1]\n",
    "        # v1: 전체 이미지를 그대로 리사이징\n",
    "        crop_patch_pil = Image.fromarray(crop_patch)\n",
    "        img = transform(crop_patch_pil)  # (3, 32, 32)\n",
    "        images.append(img)\n",
    "    images = torch.stack(images)\n",
    "    batch.append(images)\n",
    "batch = torch.stack(batch)\n",
    "\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "62c4be7e-28df-4554-abaf-647b8918afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageSetViT(nn.Module):\n",
    "    def __init__(self, patch_dim=3*32*32, embed_dim=256, num_patches=11, num_heads=4, num_classes=5, depth=4):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_dim = patch_dim\n",
    "\n",
    "        # Linear embedding: 각 이미지(=패치)를 벡터로\n",
    "        self.embedding = nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "        # Learnable positional encoding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        # Classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 11, 3, 32, 32)\n",
    "        B, N, C, H, W = x.shape\n",
    "        x = x.view(B, N, -1)  # (B, 11, 3*32*32)\n",
    "        x = self.embedding(x)  # (B, 11, embed_dim)\n",
    "        x = x + self.pos_embedding  # positional encoding\n",
    "\n",
    "        x = self.transformer(x)  # (B, 11, embed_dim)\n",
    "        x = x.mean(dim=1)  # aggregate over patches\n",
    "\n",
    "        out = self.head(x)  # (B, num_classes)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "907e9f0c-4ee4-41a4-9907-0f0dabfbd65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageSetViT(\n",
       "  (embedding): Linear(in_features=3072, out_features=256, bias=True)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=256, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ImageSetViT()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d00ebefb-15c4-465f-ad2f-7a8b5e80ca96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4527, 0.4858, 0.5246, 0.4058, 0.3553]], device='cuda:0')\n",
      "예측 결과: tensor([0., 0., 1., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = batch[0:1].to(device)\n",
    "    logits = model(x)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    print(probs)\n",
    "    preds = (probs > 0.5).float()\n",
    "\n",
    "    print(\"예측 결과:\", preds[0])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "06e4dff3-3827-4c03-b76d-9b7556fe6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    수정된 ViT 모듈:\n",
    "      - 이미지와 해당 이미지의 바운딩 박스(boxes)를 입력받습니다.\n",
    "      - 각 바운딩 박스 영역을 crop한 후 patch_size로 resize하여 패치로 사용합니다.\n",
    "      - 각 이미지에 대해, 바운딩 박스들을 y1 좌표(세로 기준)로 정렬하여 순서를 고정합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, *, patch_size, num_boxes, num_classes, dim, depth, heads, mlp_dim,\n",
    "                 pool='cls', channels=3, dim_head=64, dropout=0., emb_dropout=0.):\n",
    "        super().__init__()\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        patch_dim = channels * patch_height * patch_width  # flatten한 패치의 차원\n",
    "\n",
    "        # 각 crop된 patch를 flatten하고 embedding하는 부분\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b n c h w -> b n (c h w)'),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        # pos_embedding의 개수는 (num_boxes + 1) (cls token 포함)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_boxes + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        self.patch_size = (patch_height, patch_width)  # forward에서 resize할 크기\n",
    "\n",
    "    def forward(self, img, boxes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          img: 입력 이미지 텐서, shape (b, c, H, W)\n",
    "          boxes: 바운딩 박스 텐서, shape (b, num_boxes, 4)\n",
    "                 각 box는 (x1, y1, x2, y2) 순이며, 픽셀 좌표(정수)를 가정합니다.\n",
    "        \"\"\"\n",
    "        b = img.shape[0]\n",
    "        num_boxes = boxes.shape[1]\n",
    "        patches = []\n",
    "\n",
    "        for i in range(b):\n",
    "            img_i = img[i]  # (c, H, W)\n",
    "\n",
    "            # 각 이미지의 바운딩 박스들을 y1 좌표 기준으로 정렬 (vertical 순서 유지)\n",
    "            boxes_i = boxes[i]  # (num_boxes, 4)\n",
    "            print(boxes_i)\n",
    "            # boxes_i[:, 1]가 y1 좌표입니다.\n",
    "            sorted_indices = torch.argsort(boxes_i[:, 1])\n",
    "            boxes_i = boxes_i[sorted_indices]\n",
    "\n",
    "            patches_per_image = []\n",
    "            for box in boxes_i:\n",
    "                # (x1, y1, x2, y2) 좌표를 정수로 변환\n",
    "                x1, y1, x2, y2 = [int(coord.item()) for coord in box]\n",
    "                # 이미지 crop (채널 차원은 그대로 유지)\n",
    "                patch = img_i[:, y1:y2, x1:x2]  # (c, h_box, w_box)\n",
    "                # patch 크기가 patch_size와 다를 수 있으므로 resize\n",
    "                patch = patch.unsqueeze(0)  # (1, c, h_box, w_box)\n",
    "                patch = F.interpolate(patch, size=self.patch_size, mode='bilinear', align_corners=False)\n",
    "                patch = patch.squeeze(0)  # (c, patch_height, patch_width)\n",
    "                patches_per_image.append(patch)\n",
    "            # (num_boxes, c, patch_height, patch_width)\n",
    "            patches.append(torch.stack(patches_per_image, dim=0))\n",
    "        # (b, num_boxes, c, patch_height, patch_width)\n",
    "        patches = torch.stack(patches, dim=0)\n",
    "\n",
    "        # patch embedding: flatten 각 패치를 벡터로 변환 후 embedding\n",
    "        x = self.to_patch_embedding(patches)  # (b, num_boxes, dim)\n",
    "\n",
    "        # cls token 추가\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (b, num_boxes + 1, dim)\n",
    "\n",
    "        # 위치 임베딩 적용 (cls token 포함)\n",
    "        x = x + self.pos_embedding[:, : (num_boxes + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Transformer 통과\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # pool 방식에 따라 최종 토큰 선택\n",
    "        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e32a4d55-fa8a-414e-859f-19b6b60fb095",
   "metadata": {},
   "outputs": [],
   "source": [
    "vitmodel = ViT(\n",
    "    patch_size = 32,\n",
    "    num_boxes = 36,\n",
    "    num_classes = 2,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a3a90cfd-f731-4b4b-a05d-90ef0f3257f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.527491' '0.363526' '0.858719' '0.059048']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvitmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_xywhn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[100], line 131\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, img, boxes)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(boxes_i)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# boxes_i[:, 1]가 y1 좌표입니다.\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort(\u001b[43mboxes_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    132\u001b[0m boxes_i \u001b[38;5;241m=\u001b[39m boxes_i[sorted_indices]\n\u001b[1;32m    134\u001b[0m patches_per_image \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "vitmodel(results[0].orig_img, cur_xywhn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87914ba-1fb3-4a42-9d76-c9d048dd17f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
