{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdafc4c0-2d42-400a-aa6a-54749b48dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\" # GPU - selection\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ce307a-7c22-447f-8238-26bbc46cb46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "class UrinKitDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_size=32):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        # 이미지-레이블 경로 추출 + 유효 키트만 추려서 저장\n",
    "        self.valid_kits = []  # (img_path, label_path, class_idx, kit_index)\n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            for sub in os.listdir(cls_path):\n",
    "                sub_path = os.path.join(cls_path, sub)\n",
    "                images = glob.glob(os.path.join(sub_path, \"*.jpg\"))\n",
    "                for img_path in images:\n",
    "                    label_path = img_path.replace(\".jpg\", \".txt\")\n",
    "                    if not os.path.exists(label_path):\n",
    "                        continue\n",
    "\n",
    "                    image = Image.open(img_path).convert(\"RGB\")\n",
    "                    img_w, img_h = image.size\n",
    "                    with open(label_path, \"r\") as f:\n",
    "                        lines = list(map(str.strip, f.readlines()))\n",
    "                    labels = []\n",
    "                    for line in lines:\n",
    "                        cls_id, x, y, w, h = map(float, line.split())\n",
    "                        if cls_id != 0:\n",
    "                            continue\n",
    "                        x1 = (x - w / 2) * img_w\n",
    "                        y1 = (y - h / 2) * img_h\n",
    "                        x2 = (x + w / 2) * img_w\n",
    "                        y2 = (y + h / 2) * img_h\n",
    "                        labels.append([x1, y1, x2, y2, int(cls_id)])\n",
    "\n",
    "                    labels = np.array(labels)\n",
    "\n",
    "                    # sorted_labels = labels[np.lexsort((labels[:,1], labels[:,0]))]\n",
    "                    # kit_boxes = sorted_labels[:5]\n",
    "                    # sensor_boxes = sorted_labels[5:]\n",
    "                    # kit_boxes = kit_boxes[np.lexsort((kit_boxes[:,0], kit_boxes[:,1]))]\n",
    "\n",
    "                    for kit_idx, kit in enumerate(labels):\n",
    "                        x1, y1, x2, y2, _ = kit\n",
    "                        self.valid_kits.append((img_path, label_path, self.class_to_idx[cls], kit_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_kits)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_path, class_idx, kit_idx = self.valid_kits[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        np_image = np.array(image)\n",
    "        img_w, img_h = image.size\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            label_lines = list(map(str.strip, f.readlines()))\n",
    "\n",
    "        labels = []\n",
    "        for line in label_lines:\n",
    "            cls_id, x, y, w, h = map(float, line.split())\n",
    "            if cls_id != 0:\n",
    "                continue\n",
    "            x1 = (x - w / 2) * img_w\n",
    "            y1 = (y - h / 2) * img_h\n",
    "            x2 = (x + w / 2) * img_w\n",
    "            y2 = (y + h / 2) * img_h\n",
    "            labels.append([x1, y1, x2, y2, int(cls_id)])\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        # sorted_labels = labels[np.lexsort((labels[:,1], labels[:,0]))]\n",
    "        # kit_boxes = sorted_labels[:5]\n",
    "        # sensor_boxes = sorted_labels[5:]\n",
    "\n",
    "        # kit_boxes = kit_boxes[np.lexsort((kit_boxes[:,0], kit_boxes[:,1]))]\n",
    "        current_kit = labels[kit_idx]\n",
    "\n",
    "        x1, y1, x2, y2, _ = current_kit\n",
    "        # mask = (sensor_boxes[:, 0] > x1) & (sensor_boxes[:, 0] < x2) & \\\n",
    "        #        (sensor_boxes[:, 1] > y1) & (sensor_boxes[:, 1] < y2)\n",
    "        # group = sensor_boxes[mask]\n",
    "        # group = group[np.lexsort((group[:,1], group[:,0]))]\n",
    "\n",
    "        # patches = []\n",
    "        # for patch in group:\n",
    "        #     px1, py1, px2, py2, _ = patch\n",
    "        cropped = np_image[round(y1):round(y2), round(x1):round(x2), ::-1]\n",
    "        cropped_pil = Image.fromarray(cropped)\n",
    "        transformed = self.transform(cropped_pil)\n",
    "        # patches.append(transformed)\n",
    "\n",
    "        # X = torch.stack(patches)  # (11, 3, H, W)\n",
    "        Y = torch.tensor(class_idx)\n",
    "\n",
    "        return transformed, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b66e2f-b94f-48ef-8546-494106c9e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = UrinKitDataset(\"/home/jupyter-heypaprika/input/urin_by_class\", image_size=224)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39cb9e58-bab3-45ad-a4c7-c3ee1f6e5266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4])\n",
      "tensor([15, 22,  9,  6])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in dataloader:\n",
    "    print(X.shape)  # (4, 11, 3, 32, 32)\n",
    "    print(Y.shape)  # (4,)\n",
    "    print(Y)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf81de6f-72eb-4e3c-855b-45fe381d7253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FastVit(\n",
       "  (stem): Sequential(\n",
       "    (0): MobileOneBlock(\n",
       "      (se): Identity()\n",
       "      (conv_kxk): ModuleList(\n",
       "        (0): ConvNormAct(\n",
       "          (conv): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_scale): ConvNormAct(\n",
       "        (conv): Conv2d(3, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (1): MobileOneBlock(\n",
       "      (se): Identity()\n",
       "      (conv_kxk): ModuleList(\n",
       "        (0): ConvNormAct(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_scale): ConvNormAct(\n",
       "        (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(2, 2), groups=48, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "    (2): MobileOneBlock(\n",
       "      (se): Identity()\n",
       "      (identity): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv_kxk): ModuleList(\n",
       "        (0): ConvNormAct(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNormAct2d(\n",
       "            48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (drop): Identity()\n",
       "            (act): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (act): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): FastVitStage(\n",
       "      (downsample): Identity()\n",
       "      (pos_emb): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): RepMixerBlock(\n",
       "          (token_mixer): RepMixer(\n",
       "            (norm): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (mixer): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (conv_kxk): ModuleList(\n",
       "                (0): ConvNormAct(\n",
       "                  (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "                  (bn): BatchNormAct2d(\n",
       "                    48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                    (drop): Identity()\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (conv_scale): ConvNormAct(\n",
       "                (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "                (bn): BatchNormAct2d(\n",
       "                  48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (layer_scale): LayerScale2d()\n",
       "          )\n",
       "          (mlp): ConvMlp(\n",
       "            (conv): ConvNormAct(\n",
       "              (conv): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (fc1): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layer_scale): LayerScale2d()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): RepMixerBlock(\n",
       "          (token_mixer): RepMixer(\n",
       "            (norm): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (mixer): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (conv_kxk): ModuleList(\n",
       "                (0): ConvNormAct(\n",
       "                  (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "                  (bn): BatchNormAct2d(\n",
       "                    48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                    (drop): Identity()\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (conv_scale): ConvNormAct(\n",
       "                (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
       "                (bn): BatchNormAct2d(\n",
       "                  48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (layer_scale): LayerScale2d()\n",
       "          )\n",
       "          (mlp): ConvMlp(\n",
       "            (conv): ConvNormAct(\n",
       "              (conv): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (fc1): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layer_scale): LayerScale2d()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): FastVitStage(\n",
       "      (downsample): PatchEmbed(\n",
       "        (proj): Sequential(\n",
       "          (0): ReparamLargeKernelConv(\n",
       "            (large_conv): ConvNormAct(\n",
       "              (conv): Conv2d(48, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=48, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (small_conv): ConvNormAct(\n",
       "              (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (1): MobileOneBlock(\n",
       "            (se): Identity()\n",
       "            (identity): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv_kxk): ModuleList(\n",
       "              (0): ConvNormAct(\n",
       "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNormAct2d(\n",
       "                  96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pos_emb): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): RepMixerBlock(\n",
       "          (token_mixer): RepMixer(\n",
       "            (norm): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (mixer): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (conv_kxk): ModuleList(\n",
       "                (0): ConvNormAct(\n",
       "                  (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                  (bn): BatchNormAct2d(\n",
       "                    96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                    (drop): Identity()\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (conv_scale): ConvNormAct(\n",
       "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "                (bn): BatchNormAct2d(\n",
       "                  96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (layer_scale): LayerScale2d()\n",
       "          )\n",
       "          (mlp): ConvMlp(\n",
       "            (conv): ConvNormAct(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (fc1): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layer_scale): LayerScale2d()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): RepMixerBlock(\n",
       "          (token_mixer): RepMixer(\n",
       "            (norm): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (mixer): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (conv_kxk): ModuleList(\n",
       "                (0): ConvNormAct(\n",
       "                  (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "                  (bn): BatchNormAct2d(\n",
       "                    96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                    (drop): Identity()\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (conv_scale): ConvNormAct(\n",
       "                (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
       "                (bn): BatchNormAct2d(\n",
       "                  96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (layer_scale): LayerScale2d()\n",
       "          )\n",
       "          (mlp): ConvMlp(\n",
       "            (conv): ConvNormAct(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (fc1): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layer_scale): LayerScale2d()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): FastVitStage(\n",
       "      (downsample): PatchEmbed(\n",
       "        (proj): Sequential(\n",
       "          (0): ReparamLargeKernelConv(\n",
       "            (large_conv): ConvNormAct(\n",
       "              (conv): Conv2d(96, 192, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (small_conv): ConvNormAct(\n",
       "              (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (1): MobileOneBlock(\n",
       "            (se): Identity()\n",
       "            (identity): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv_kxk): ModuleList(\n",
       "              (0): ConvNormAct(\n",
       "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNormAct2d(\n",
       "                  192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pos_emb): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): RepMixerBlock(\n",
       "          (token_mixer): RepMixer(\n",
       "            (norm): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (mixer): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (conv_kxk): ModuleList(\n",
       "                (0): ConvNormAct(\n",
       "                  (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                  (bn): BatchNormAct2d(\n",
       "                    192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                    (drop): Identity()\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (conv_scale): ConvNormAct(\n",
       "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "                (bn): BatchNormAct2d(\n",
       "                  192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (layer_scale): LayerScale2d()\n",
       "          )\n",
       "          (mlp): ConvMlp(\n",
       "            (conv): ConvNormAct(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (fc1): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layer_scale): LayerScale2d()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): RepMixerBlock(\n",
       "          (token_mixer): RepMixer(\n",
       "            (norm): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (mixer): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (conv_kxk): ModuleList(\n",
       "                (0): ConvNormAct(\n",
       "                  (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                  (bn): BatchNormAct2d(\n",
       "                    192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                    (drop): Identity()\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (conv_scale): ConvNormAct(\n",
       "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "                (bn): BatchNormAct2d(\n",
       "                  192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (layer_scale): LayerScale2d()\n",
       "          )\n",
       "          (mlp): ConvMlp(\n",
       "            (conv): ConvNormAct(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (fc1): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layer_scale): LayerScale2d()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): RepMixerBlock(\n",
       "          (token_mixer): RepMixer(\n",
       "            (norm): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (mixer): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (conv_kxk): ModuleList(\n",
       "                (0): ConvNormAct(\n",
       "                  (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                  (bn): BatchNormAct2d(\n",
       "                    192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                    (drop): Identity()\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (conv_scale): ConvNormAct(\n",
       "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "                (bn): BatchNormAct2d(\n",
       "                  192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (layer_scale): LayerScale2d()\n",
       "          )\n",
       "          (mlp): ConvMlp(\n",
       "            (conv): ConvNormAct(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (fc1): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layer_scale): LayerScale2d()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): RepMixerBlock(\n",
       "          (token_mixer): RepMixer(\n",
       "            (norm): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (mixer): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (conv_kxk): ModuleList(\n",
       "                (0): ConvNormAct(\n",
       "                  (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                  (bn): BatchNormAct2d(\n",
       "                    192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                    (drop): Identity()\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (conv_scale): ConvNormAct(\n",
       "                (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
       "                (bn): BatchNormAct2d(\n",
       "                  192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (layer_scale): LayerScale2d()\n",
       "          )\n",
       "          (mlp): ConvMlp(\n",
       "            (conv): ConvNormAct(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (fc1): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layer_scale): LayerScale2d()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): FastVitStage(\n",
       "      (downsample): PatchEmbed(\n",
       "        (proj): Sequential(\n",
       "          (0): ReparamLargeKernelConv(\n",
       "            (large_conv): ConvNormAct(\n",
       "              (conv): Conv2d(192, 384, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=192, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (small_conv): ConvNormAct(\n",
       "              (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (act): Identity()\n",
       "          )\n",
       "          (1): MobileOneBlock(\n",
       "            (se): Identity()\n",
       "            (identity): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (conv_kxk): ModuleList(\n",
       "              (0): ConvNormAct(\n",
       "                (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (bn): BatchNormAct2d(\n",
       "                  384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pos_emb): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): RepMixerBlock(\n",
       "          (token_mixer): RepMixer(\n",
       "            (norm): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (mixer): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (conv_kxk): ModuleList(\n",
       "                (0): ConvNormAct(\n",
       "                  (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                  (bn): BatchNormAct2d(\n",
       "                    384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                    (drop): Identity()\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (conv_scale): ConvNormAct(\n",
       "                (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
       "                (bn): BatchNormAct2d(\n",
       "                  384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (layer_scale): LayerScale2d()\n",
       "          )\n",
       "          (mlp): ConvMlp(\n",
       "            (conv): ConvNormAct(\n",
       "              (conv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (fc1): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layer_scale): LayerScale2d()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): RepMixerBlock(\n",
       "          (token_mixer): RepMixer(\n",
       "            (norm): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (mixer): MobileOneBlock(\n",
       "              (se): Identity()\n",
       "              (identity): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (conv_kxk): ModuleList(\n",
       "                (0): ConvNormAct(\n",
       "                  (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                  (bn): BatchNormAct2d(\n",
       "                    384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                    (drop): Identity()\n",
       "                    (act): Identity()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (conv_scale): ConvNormAct(\n",
       "                (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
       "                (bn): BatchNormAct2d(\n",
       "                  384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (act): Identity()\n",
       "            )\n",
       "            (layer_scale): LayerScale2d()\n",
       "          )\n",
       "          (mlp): ConvMlp(\n",
       "            (conv): ConvNormAct(\n",
       "              (conv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
       "              (bn): BatchNormAct2d(\n",
       "                384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "            )\n",
       "            (fc1): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layer_scale): LayerScale2d()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_conv): MobileOneBlock(\n",
       "    (se): SEModule(\n",
       "      (fc1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bn): Identity()\n",
       "      (act): ReLU(inplace=True)\n",
       "      (fc2): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (gate): Sigmoid()\n",
       "    )\n",
       "    (conv_kxk): ModuleList(\n",
       "      (0): ConvNormAct(\n",
       "        (conv): Conv2d(384, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_scale): ConvNormAct(\n",
       "      (conv): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
       "      (bn): BatchNormAct2d(\n",
       "        768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "        (drop): Identity()\n",
       "        (act): Identity()\n",
       "      )\n",
       "    )\n",
       "    (act): GELU(approximate='none')\n",
       "  )\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=768, out_features=25, bias=True)\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn  # Neural network module\n",
    "\n",
    "# 1. 사전학습된 FastViT 모델 로드\n",
    "model = timm.create_model('fastvit_t8', pretrained=True)\n",
    "# 2. 자신의 데이터셋 클래스 수에 맞게 마지막 레이어 변경\n",
    "num_classes = 25  # 예: 10개 클래스 분류\n",
    "model.reset_classifier(num_classes)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1585941-67bf-4dbd-9a63-faafeac4b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageSetViT(nn.Module):\n",
    "    def __init__(self, patch_dim=3*32*32, embed_dim=256, num_patches=11, num_heads=4, num_classes=25, depth=4):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_dim = patch_dim\n",
    "\n",
    "        # Linear embedding: 각 이미지(=패치)를 벡터로\n",
    "        self.embedding = nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "        # Learnable positional encoding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        # Classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 11, 3, 32, 32)\n",
    "        B, N, C, H, W = x.shape\n",
    "        x = x.view(B, N, -1)  # (B, 11, 3*32*32)\n",
    "        x = self.embedding(x)  # (B, 11, embed_dim)\n",
    "        x = x + self.pos_embedding  # positional encoding\n",
    "\n",
    "        x = self.transformer(x)  # (B, 11, embed_dim)\n",
    "        x = x.mean(dim=1)  # aggregate over patches\n",
    "\n",
    "        out = self.head(x)  # (B, num_classes)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f4e272-7c89-4c5f-a694-08de0e0b184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 모델 예시 (사용자 모델 정의가 이미 완료되어 있다고 가정)\n",
    "# from model import ImageSetViT  \n",
    "# from dataset import UrinKitDataset\n",
    "model_save_dir = './files'\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=10, lr=1e-4):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for X, Y in tqdm(train_loader, desc=f\"[Train Epoch {epoch+1}]\"):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            # (B, 11, 3, 32, 32) → (B, 11*3*32*32)\n",
    "            # B, P, C, H, W = X.shape\n",
    "            # X = X.view(B, -1)  # Flatten 11 patches\n",
    "\n",
    "            outputs = model(X)  # Shape: (B, num_classes)\n",
    "            loss = criterion(outputs, Y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == Y).sum().item()\n",
    "            total += Y.size(0)\n",
    "\n",
    "        acc = correct / total * 100\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {total_loss:.4f} | Train Acc: {acc:.2f}%\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X, Y in val_loader:\n",
    "                X, Y = X.to(device), Y.to(device)\n",
    "                # B, P, C, H, W = X.shape\n",
    "                # X = X.view(B, -1)\n",
    "                outputs = model(X)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == Y).sum().item()\n",
    "                total += Y.size(0)\n",
    "\n",
    "        val_acc = correct / total * 100\n",
    "        print(f\"Epoch {epoch+1}: Validation Accuracy: {val_acc:.2f}%\\n\")\n",
    "\n",
    "        model_save_path = os.path.join(model_save_dir, f\"vit_model_weight_+{epoch}epoch_{val_acc:.2f}.pth\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f'Model saved to {model_save_path}.')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f2d922d-c375-4d21-99a1-6e9f288cb8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 1]: 100%|██████████| 399/399 [10:31<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 908.0582 | Train Acc: 28.93%\n",
      "Epoch 1: Validation Accuracy: 54.76%\n",
      "\n",
      "Model saved to ./files/vit_model_weight_+0epoch_54.76.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 2]: 100%|██████████| 399/399 [10:29<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 453.3245 | Train Acc: 58.29%\n",
      "Epoch 2: Validation Accuracy: 63.77%\n",
      "\n",
      "Model saved to ./files/vit_model_weight_+1epoch_63.77.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 3]: 100%|██████████| 399/399 [10:27<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 299.7276 | Train Acc: 72.74%\n",
      "Epoch 3: Validation Accuracy: 74.04%\n",
      "\n",
      "Model saved to ./files/vit_model_weight_+2epoch_74.04.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 4]: 100%|██████████| 399/399 [10:33<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 213.5237 | Train Acc: 80.97%\n",
      "Epoch 4: Validation Accuracy: 74.46%\n",
      "\n",
      "Model saved to ./files/vit_model_weight_+3epoch_74.46.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 5]: 100%|██████████| 399/399 [10:30<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 146.8643 | Train Acc: 87.99%\n",
      "Epoch 5: Validation Accuracy: 85.89%\n",
      "\n",
      "Model saved to ./files/vit_model_weight_+4epoch_85.89.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 6]:  95%|█████████▍| 378/399 [10:03<00:33,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Validation Accuracy: 82.97%\n",
      "\n",
      "Model saved to ./files/vit_model_weight_+5epoch_82.97.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 7]: 100%|██████████| 399/399 [10:30<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 80.1730 | Train Acc: 93.57%\n",
      "Epoch 7: Validation Accuracy: 92.32%\n",
      "\n",
      "Model saved to ./files/vit_model_weight_+6epoch_92.32.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 8]: 100%|██████████| 399/399 [10:20<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 61.2149 | Train Acc: 95.05%\n",
      "Epoch 8: Validation Accuracy: 90.57%\n",
      "\n",
      "Model saved to ./files/vit_model_weight_+7epoch_90.57.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 9]: 100%|██████████| 399/399 [10:20<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 48.8742 | Train Acc: 95.99%\n",
      "Epoch 9: Validation Accuracy: 89.98%\n",
      "\n",
      "Model saved to ./files/vit_model_weight_+8epoch_89.98.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 10]: 100%|██████████| 399/399 [10:35<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 35.3400 | Train Acc: 97.18%\n",
      "Epoch 10: Validation Accuracy: 83.89%\n",
      "\n",
      "Model saved to ./files/vit_model_weight_+9epoch_83.89.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 11]: 100%|██████████| 399/399 [10:14<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 47.7336 | Train Acc: 96.12%\n",
      "Epoch 11: Validation Accuracy: 85.81%\n",
      "\n",
      "Model saved to ./files/vit_model_weight_+10epoch_85.81.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 12]: 100%|██████████| 399/399 [10:23<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 45.5698 | Train Acc: 96.39%\n",
      "Epoch 12: Validation Accuracy: 90.73%\n",
      "\n",
      "Model saved to ./files/vit_model_weight_+11epoch_90.73.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 13]: 100%|██████████| 399/399 [10:25<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 25.4112 | Train Acc: 98.08%\n",
      "Epoch 13: Validation Accuracy: 91.82%\n",
      "\n",
      "Model saved to ./files/vit_model_weight_+12epoch_91.82.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 14]:  43%|████▎     | 170/399 [04:29<05:54,  1.55s/it]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "dataset = UrinKitDataset(\"/home/jupyter-heypaprika/input/urin_by_class\", image_size=224)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=12, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=12)\n",
    "\n",
    "# model = ImageSetViT()  \n",
    "# model.load_state_dict(torch.load('./files/vit_custom_model_weight_eth_100_2.pth', map_location=device))\n",
    "\n",
    "trained_model = train_model(model, train_loader, val_loader, device, num_epochs=50, lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6668dd4e-8330-4afa-bd30-7737145bd24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./files/vit_model_weight.pth.\n"
     ]
    }
   ],
   "source": [
    "model_save_dir = './files'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "model_save_path = os.path.join(model_save_dir, 'vit_model_weight.pth')\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f'Model saved to {model_save_path}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24165527-1358-4a1e-9666-7a508bdb8221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 1]: 100%|██████████| 290/290 [08:48<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 135.3117 | Train Acc: 79.83%\n",
      "Epoch 1: Validation Accuracy: 73.96%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 2]: 100%|██████████| 290/290 [08:32<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 118.3377 | Train Acc: 81.62%\n",
      "Epoch 2: Validation Accuracy: 79.84%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 3]: 100%|██████████| 290/290 [08:37<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 123.0730 | Train Acc: 81.53%\n",
      "Epoch 3: Validation Accuracy: 82.14%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 4]: 100%|██████████| 290/290 [08:35<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 111.6628 | Train Acc: 82.48%\n",
      "Epoch 4: Validation Accuracy: 79.72%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 5]: 100%|██████████| 290/290 [08:24<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 121.4959 | Train Acc: 81.10%\n",
      "Epoch 5: Validation Accuracy: 76.38%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 6]: 100%|██████████| 290/290 [08:29<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 121.0709 | Train Acc: 81.36%\n",
      "Epoch 6: Validation Accuracy: 80.65%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 7]: 100%|██████████| 290/290 [08:36<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 113.9777 | Train Acc: 82.34%\n",
      "Epoch 7: Validation Accuracy: 79.38%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 8]: 100%|██████████| 290/290 [08:33<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 113.4308 | Train Acc: 82.69%\n",
      "Epoch 8: Validation Accuracy: 81.22%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 9]: 100%|██████████| 290/290 [08:26<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 116.9395 | Train Acc: 83.35%\n",
      "Epoch 9: Validation Accuracy: 80.88%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 10]: 100%|██████████| 290/290 [08:31<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 109.4477 | Train Acc: 82.66%\n",
      "Epoch 10: Validation Accuracy: 77.88%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 11]: 100%|██████████| 290/290 [08:31<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 101.9800 | Train Acc: 84.18%\n",
      "Epoch 11: Validation Accuracy: 85.37%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 12]: 100%|██████████| 290/290 [08:38<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 108.2597 | Train Acc: 83.61%\n",
      "Epoch 12: Validation Accuracy: 84.10%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 13]: 100%|██████████| 290/290 [08:38<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 114.2033 | Train Acc: 81.99%\n",
      "Epoch 13: Validation Accuracy: 80.88%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 14]: 100%|██████████| 290/290 [08:31<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 107.3334 | Train Acc: 83.61%\n",
      "Epoch 14: Validation Accuracy: 82.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 15]: 100%|██████████| 290/290 [08:47<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 103.3451 | Train Acc: 84.30%\n",
      "Epoch 15: Validation Accuracy: 81.57%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 16]: 100%|██████████| 290/290 [08:30<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 106.5660 | Train Acc: 84.44%\n",
      "Epoch 16: Validation Accuracy: 79.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 17]: 100%|██████████| 290/290 [08:34<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 95.4424 | Train Acc: 85.11%\n",
      "Epoch 17: Validation Accuracy: 85.25%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 18]: 100%|██████████| 290/290 [08:48<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 122.9341 | Train Acc: 81.82%\n",
      "Epoch 18: Validation Accuracy: 84.10%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 19]: 100%|██████████| 290/290 [08:28<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 94.8844 | Train Acc: 85.34%\n",
      "Epoch 19: Validation Accuracy: 79.61%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 20]: 100%|██████████| 290/290 [08:25<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 94.4772 | Train Acc: 84.99%\n",
      "Epoch 20: Validation Accuracy: 82.60%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 21]: 100%|██████████| 290/290 [08:38<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 113.5547 | Train Acc: 83.23%\n",
      "Epoch 21: Validation Accuracy: 78.92%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 22]: 100%|██████████| 290/290 [08:38<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss: 93.4381 | Train Acc: 85.34%\n",
      "Epoch 22: Validation Accuracy: 81.80%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 23]: 100%|██████████| 290/290 [08:31<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss: 97.8417 | Train Acc: 85.28%\n",
      "Epoch 23: Validation Accuracy: 85.14%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 24]: 100%|██████████| 290/290 [08:24<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss: 97.4854 | Train Acc: 86.14%\n",
      "Epoch 24: Validation Accuracy: 78.46%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 25]: 100%|██████████| 290/290 [08:32<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss: 96.3670 | Train Acc: 86.14%\n",
      "Epoch 25: Validation Accuracy: 83.18%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 26]: 100%|██████████| 290/290 [08:50<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss: 85.5443 | Train Acc: 87.53%\n",
      "Epoch 26: Validation Accuracy: 82.83%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 27]: 100%|██████████| 290/290 [08:36<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss: 100.1750 | Train Acc: 85.54%\n",
      "Epoch 27: Validation Accuracy: 83.99%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 28]: 100%|██████████| 290/290 [08:41<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss: 91.8145 | Train Acc: 86.95%\n",
      "Epoch 28: Validation Accuracy: 85.48%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 29]: 100%|██████████| 290/290 [08:39<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss: 84.4352 | Train Acc: 87.24%\n",
      "Epoch 29: Validation Accuracy: 83.53%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 30]: 100%|██████████| 290/290 [08:43<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss: 93.6645 | Train Acc: 86.11%\n",
      "Epoch 30: Validation Accuracy: 82.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 31]: 100%|██████████| 290/290 [08:47<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss: 91.2231 | Train Acc: 86.98%\n",
      "Epoch 31: Validation Accuracy: 87.90%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 32]: 100%|██████████| 290/290 [08:27<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Loss: 86.1245 | Train Acc: 87.21%\n",
      "Epoch 32: Validation Accuracy: 78.80%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 33]: 100%|██████████| 290/290 [08:34<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Loss: 97.7741 | Train Acc: 86.03%\n",
      "Epoch 33: Validation Accuracy: 82.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 34]: 100%|██████████| 290/290 [08:38<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss: 80.5526 | Train Acc: 88.19%\n",
      "Epoch 34: Validation Accuracy: 83.41%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 35]: 100%|██████████| 290/290 [08:36<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Train Loss: 90.5141 | Train Acc: 86.83%\n",
      "Epoch 35: Validation Accuracy: 86.18%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 36]: 100%|██████████| 290/290 [08:27<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Train Loss: 83.1187 | Train Acc: 87.47%\n",
      "Epoch 36: Validation Accuracy: 86.87%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 37]: 100%|██████████| 290/290 [08:35<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: Train Loss: 86.5858 | Train Acc: 87.76%\n",
      "Epoch 37: Validation Accuracy: 85.37%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 38]: 100%|██████████| 290/290 [08:45<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Train Loss: 80.6323 | Train Acc: 87.67%\n",
      "Epoch 38: Validation Accuracy: 84.91%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 39]: 100%|██████████| 290/290 [08:44<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Train Loss: 79.7460 | Train Acc: 88.56%\n",
      "Epoch 39: Validation Accuracy: 85.02%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 40]: 100%|██████████| 290/290 [08:50<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Train Loss: 84.6997 | Train Acc: 87.70%\n",
      "Epoch 40: Validation Accuracy: 79.03%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 41]: 100%|██████████| 290/290 [08:31<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Train Loss: 76.0156 | Train Acc: 88.76%\n",
      "Epoch 41: Validation Accuracy: 87.79%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 42]: 100%|██████████| 290/290 [08:41<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: Train Loss: 80.3622 | Train Acc: 88.36%\n",
      "Epoch 42: Validation Accuracy: 79.84%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 43]: 100%|██████████| 290/290 [08:49<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: Train Loss: 95.8277 | Train Acc: 86.55%\n",
      "Epoch 43: Validation Accuracy: 85.14%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 44]: 100%|██████████| 290/290 [08:35<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: Train Loss: 77.5271 | Train Acc: 88.88%\n",
      "Epoch 44: Validation Accuracy: 87.10%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 45]: 100%|██████████| 290/290 [08:43<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Loss: 70.4427 | Train Acc: 90.03%\n",
      "Epoch 45: Validation Accuracy: 85.60%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 46]: 100%|██████████| 290/290 [08:41<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: Train Loss: 70.6699 | Train Acc: 89.46%\n",
      "Epoch 46: Validation Accuracy: 88.82%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 47]: 100%|██████████| 290/290 [08:51<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss: 97.9500 | Train Acc: 86.06%\n",
      "Epoch 47: Validation Accuracy: 79.38%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 48]: 100%|██████████| 290/290 [08:53<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Loss: 69.7971 | Train Acc: 89.89%\n",
      "Epoch 48: Validation Accuracy: 82.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 49]: 100%|██████████| 290/290 [10:09<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss: 71.8248 | Train Acc: 89.11%\n",
      "Epoch 49: Validation Accuracy: 88.48%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 50]: 100%|██████████| 290/290 [09:17<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss: 77.8933 | Train Acc: 88.62%\n",
      "Epoch 50: Validation Accuracy: 87.44%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = ImageSetViT() \n",
    "# + 추가 학습\n",
    "trained_model = train_model(model, train_loader, val_loader, device, num_epochs=50, lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae190d0a-7818-45e7-b5f3-99cb63c85ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./files/vit_custom_model_weight_eth_100_2.pth.\n"
     ]
    }
   ],
   "source": [
    "model_save_dir = './files'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "model_save_path = os.path.join(model_save_dir, 'vit_custom_model_weight_eth_100_2.pth')\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f'Model saved to {model_save_path}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03acd4b9-c7ce-4cc4-b7d5-624315a9df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ImageSetViT() \n",
    "# + 추가 학습\n",
    "trained_model = train_model(model, train_loader, val_loader, device, num_epochs=100, lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754b2ba-c7c0-4468-bab5-3cedb0a97a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageSetViT()  \n",
    "trained_model = train_model(model, train_loader, val_loader, device, num_epochs=30, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba9d8e-1d48-474a-9b61-68ffb282712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. object detection 패치 단위로 안하고, 키트만 detection 해서, ViT 모델에 키트 데이터 넣을 경우 어떻게 결과가 나올지?\n",
    "\n",
    "# dataset 정의\n",
    "\n",
    "# ViT 모델 로드\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0736c-1af9-4ae1-a8da-723d28212260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. classical detection (opencv, 색상 기반)\n",
    "\n",
    "# object detection으로 패치에 대해서 detection하고, color 기반으로 휴리스틱하게 판정을 내리기?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a36cb436-bd66-4a85-aa57-7734a35f164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0 0.550258 0.482883 0.792669 0.077674', '1 0.617799 0.486476 0.031260 0.037913', '1 0.329770 0.677449 0.032549 0.039392', '1 0.333755 0.299361 0.033366 0.040691', '0 0.549963 0.680188 0.791086 0.061061', '1 0.525079 0.300167 0.033444 0.041095', '1 0.187318 0.674683 0.032913 0.038827', '1 0.472916 0.680706 0.032978 0.040174', '1 0.572871 0.300507 0.033263 0.040777', '1 0.188825 0.470396 0.032049 0.037992', '1 0.381804 0.299490 0.033687 0.041507', '1 0.520465 0.681194 0.032542 0.039607', '1 0.235907 0.471770 0.032014 0.037578', '1 0.237901 0.298994 0.033299 0.040819', '1 0.615836 0.683158 0.032793 0.040101', '1 0.477110 0.299988 0.033037 0.040756', '1 0.620210 0.300888 0.032191 0.040185', '1 0.331899 0.475316 0.030721 0.036569', '1 0.190821 0.299361 0.033619 0.040581', '1 0.475174 0.480683 0.030734 0.036728', '1 0.283940 0.473679 0.031935 0.037647', '1 0.281818 0.676706 0.032514 0.039516', '0 0.552543 0.300242 0.789979 0.051366', '1 0.567939 0.681674 0.032169 0.039883', '1 0.429138 0.299749 0.033448 0.040841', '1 0.666545 0.301228 0.032204 0.039594', '1 0.570191 0.484493 0.031066 0.037149', '1 0.234459 0.675842 0.032282 0.038217', '1 0.377600 0.678471 0.032906 0.039224', '1 0.664757 0.488739 0.031481 0.038117', '1 0.379915 0.477053 0.030825 0.036873', '1 0.285611 0.299239 0.033681 0.040980', '1 0.522610 0.482676 0.030904 0.037218', '1 0.425060 0.679365 0.031918 0.038946', '1 0.663309 0.683811 0.032748 0.040239', '1 0.428272 0.479201 0.030333 0.036825']\n",
      "cur Kit 에 해당하는 sensor 데이터:\n",
      "[696.046  837.2115 830.522  958.9545   1.    ]\n",
      "[8.850060e+02 8.357535e+02 1.018202e+03 9.582105e+02 1.000000e+00]\n",
      "[1.075082e+03 8.362470e+02 1.209806e+03 9.591870e+02 1.000000e+00]\n",
      "[1.268288e+03 8.370465e+02 1.401752e+03 9.591195e+02 1.000000e+00]\n",
      "[1.459842e+03 8.362095e+02 1.594590e+03 9.607305e+02 1.000000e+00]\n",
      "[1.649656e+03 8.379855e+02 1.783448e+03 9.605085e+02 1.000000e+00]\n",
      "[1.842366e+03 8.388300e+02 1.974514e+03 9.610980e+02 1.000000e+00]\n",
      "[2.033428e+03 8.388585e+02 2.167204e+03 9.621435e+02 1.000000e+00]\n",
      "[2.224958e+03 8.403555e+02 2.358010e+03 9.626865e+02 1.000000e+00]\n",
      "[2.416458e+03 8.423865e+02 2.545222e+03 9.629415e+02 1.000000e+00]\n",
      "[2.601772e+03 8.442930e+02 2.730588e+03 9.630750e+02 1.000000e+00]\n",
      "cur Kit 에 해당하는 sensor 데이터:\n",
      "[6.912020e+02 1.354200e+03 8.193980e+02 1.468176e+03 1.000000e+00]\n",
      "[8.796000e+02 1.358943e+03 1.007656e+03 1.471677e+03 1.000000e+00]\n",
      "[1.0718900e+03 1.3645665e+03 1.1996300e+03 1.4775075e+03 1.0000000e+00]\n",
      "[1.2661540e+03 1.3710945e+03 1.3890380e+03 1.4808015e+03 1.0000000e+00]\n",
      "[1.4580100e+03 1.3758495e+03 1.5813100e+03 1.4864685e+03 1.0000000e+00]\n",
      "[1.6524220e+03 1.3823655e+03 1.7737540e+03 1.4928405e+03 1.0000000e+00]\n",
      "[1.839228e+03 1.386957e+03 1.962164e+03 1.497141e+03 1.000000e+00]\n",
      "[2.028632e+03 1.392201e+03 2.152248e+03 1.503855e+03 1.000000e+00]\n",
      "[2.2186320e+03 1.3977555e+03 2.3428960e+03 1.5092025e+03 1.0000000e+00]\n",
      "[2.4086760e+03 1.4025585e+03 2.5337160e+03 1.5162975e+03 1.0000000e+00]\n",
      "[2.5960660e+03 1.4090415e+03 2.7219900e+03 1.5233925e+03 1.0000000e+00]\n",
      "cur Kit 에 해당하는 sensor 데이터:\n",
      "[6.8344600e+02 1.9658085e+03 8.1509800e+02 2.0822895e+03 1.0000000e+00]\n",
      "[8.7327200e+02 1.9702005e+03 1.0024000e+03 2.0848515e+03 1.0000000e+00]\n",
      "[1.062244e+03 1.970844e+03 1.192300e+03 2.089392e+03 1.000000e+00]\n",
      "[1.253982e+03 1.973259e+03 1.384178e+03 2.091435e+03 1.000000e+00]\n",
      "[1.444588e+03 1.976577e+03 1.576212e+03 2.094249e+03 1.000000e+00]\n",
      "[1.636404e+03 1.979676e+03 1.764076e+03 2.096514e+03 1.000000e+00]\n",
      "[1.825708e+03 1.981857e+03 1.957620e+03 2.102379e+03 1.000000e+00]\n",
      "[2.0167760e+03 1.9841715e+03 2.1469440e+03 2.1029925e+03 1.0000000e+00]\n",
      "[2.2074180e+03 1.9851975e+03 2.3360940e+03 2.1048465e+03 1.0000000e+00]\n",
      "[2.3977580e+03 1.9893225e+03 2.5289300e+03 2.1096255e+03 1.0000000e+00]\n",
      "[2.5877400e+03 1.9910745e+03 2.7187320e+03 2.1117915e+03 1.0000000e+00]\n",
      "torch.Size([3, 11, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "## ViT 모델 학습시 코드 뼈대 작성\n",
    "## 학습 :\n",
    "## object detection 모델에 사용되는 label이 X data를 구성하는 데에 사용됨.\n",
    "## dir 이름이 Y data를 구성하는 데에 사용됨.\n",
    "\n",
    "## 검증 :\n",
    "## 학습과 동일\n",
    "\n",
    "## 참고) 테스트 :\n",
    "## object detection 모델의 예측 결과가 X data를 구성하는 데에 직접적으로 사용됨\n",
    "## Y data는 모델의 예측 결과\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "data_path = \"/home/jupyter-heypaprika/datasets/urin_check_dataset/Z-FLIP_unlabelled/\"\n",
    "files = glob.glob(data_path+\"*.txt\")\n",
    "image_file_size = glob.glob(data_path+\"*.jpg\")[0]\n",
    "image = Image.open(image_file_size).convert(\"RGB\")\n",
    "np_image = np.array(image)\n",
    "\n",
    "img_w, img_h = image.size\n",
    "\n",
    "### X : image to tensor ###\n",
    "\n",
    "with open(files[1], \"r\") as file:\n",
    "    cur_label_xywhn = list(map(str.rstrip, file.readlines()))\n",
    "\n",
    "print(cur_label_xywhn)\n",
    "\n",
    "# to transform xywhn to xyxy\n",
    "def xywhn2xyxy(x, y, w, h, img_w, img_h):\n",
    "    x1 = (x - w / 2) * img_w\n",
    "    y1 = (y - h / 2) * img_h\n",
    "    x2 = (x + w / 2) * img_w\n",
    "    y2 = (y + h / 2) * img_h\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "cur_label = [\"\" for _ in range(len(cur_label_xywhn))]\n",
    "\n",
    "for i in range(len(cur_label)):\n",
    "    n, x, y, w, h = map(float, cur_label_xywhn[i].split())\n",
    "    x1, y1, x2, y2 = xywhn2xyxy(x, y, w, h, img_w, img_h)\n",
    "    cur_label[i] = [x1, y1, x2, y2, int(n)]\n",
    "\n",
    "\n",
    "xyxy = np.array(cur_label)\n",
    "sorted_data = xyxy[np.lexsort((xyxy[:,1], xyxy[:,0]))]\n",
    "\n",
    "\n",
    "kit = sorted_data[:3]\n",
    "sensor = sorted_data[3:]\n",
    "kit = kit[np.lexsort((kit[:,0], kit[:,1]))]\n",
    "groups = []\n",
    "\n",
    "for k in kit:\n",
    "    # k[0]: lower bound for x, k[2]: upper bound for x\n",
    "    # k[1]: lower bound for y, k[3]: upper bound for y\n",
    "    mask = (sensor[:, 0] > k[0]) & (sensor[:, 0] < k[2]) & \\\n",
    "           (sensor[:, 1] > k[1]) & (sensor[:, 1] < k[3])\n",
    "    result = sensor[mask]\n",
    "    result = result[np.lexsort((result[:,1], result[:,0]))]\n",
    "\n",
    "    groups.append(result)\n",
    "\n",
    "\n",
    "image_size = 32\n",
    "\n",
    "transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),  # Converts to (C, H, W)\n",
    "        ])\n",
    "\n",
    "\n",
    "# 각 그룹 출력\n",
    "batch = []\n",
    "for group in groups:\n",
    "    print(f\"cur Kit 에 해당하는 sensor 데이터:\")\n",
    "    images = []\n",
    "    for j, patch in enumerate(group):\n",
    "        print(patch)\n",
    "        x1, y1, x2, y2, n = patch\n",
    "        crop_patch = np_image[round(y1):round(y2), round(x1):round(x2)]\n",
    "        crop_patch = crop_patch[..., ::-1]\n",
    "        # v1: 전체 이미지를 그대로 리사이징\n",
    "        crop_patch_pil = Image.fromarray(crop_patch)\n",
    "        img = transform(crop_patch_pil)  # (3, 32, 32)\n",
    "        images.append(img)\n",
    "    images = torch.stack(images)\n",
    "    batch.append(images)\n",
    "batch = torch.stack(batch)\n",
    "\n",
    "print(batch.shape)\n",
    "\n",
    "### X : image to tensor, Done ###\n",
    "\n",
    "\n",
    "\n",
    "### Y : dir to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12326d10-81a4-4a76-9955-aceda56f6dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
